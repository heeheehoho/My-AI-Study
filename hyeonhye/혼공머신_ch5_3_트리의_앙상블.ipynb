{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOj21qZs0WvCGtFtnrMvYDf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heeheehoho/My-AI-Study/blob/main/hyeonhye/%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0_ch5_3_%ED%8A%B8%EB%A6%AC%EC%9D%98_%EC%95%99%EC%83%81%EB%B8%94.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<혼자 공부하는 머신러닝+딥러닝 5장 트리 알고리즘 - 3: 트리의 앙상블> </br>\n",
        "앙상블 학습이 무엇인지 이해하고 다양한 학습 알고리즘을 실습을 통해 배운다."
      ],
      "metadata": {
        "id": "z5o3YxeOq21u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"나무 말고 숲을 봐.\" </br>\n",
        "혼공머신이 '머신러닝', '나무', '숲'을 검색하니 랜덤 포레스트(Random Forest)라는 알고리즘이 보인다. </br>\n",
        "가장 좋은 알고리즘이 있다고 해서 다른 알고리즘을 배울 필요가 없는 것은 아니다. 보편적으로 성능이 좋아 널리 사용되는 알고리즘이 있지만 문제마다 다를 수 있으며 어떤 알고리즘이 더 뛰어나다고 미리 판단해서는 안 된다."
      ],
      "metadata": {
        "id": "KMgfTjTXriJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 정형 데이터와 비정형 데이터\n",
        "  - 정형 데이터(structed data): 어떤 구조로 되어 있다.\n",
        "    - 이런 데이터는 CSV나 데이터베이스(Database), 혹은 엑셀(Excel)에 저장하기 쉽다.\n",
        "    - 온라인 쇼핑몰에 진열된 상품과 우리가 구매한 쇼핑 정보는 모두 데이터베이스에 저장되는 정형 데이터에 속한다. 사실 프로그래머가 다루는 대부분의 데이터가 정형 데이터이다.\n",
        "  - 정형데이터와 반대되는 데이터를 비정형 데이터(unstructed data)라고 부른다.\n",
        "    - 비정형 데이터는 데이터베이스나 엑셀로 표현하기 어려운 것들이다.\n",
        "    - 책의 글과 같은 텍스트 데이터, 디지털 카메라로 찍은 사진, 핸드폰으로 듣는 디지털 음악 등이 있다."
      ],
      "metadata": {
        "id": "qtvDLeBQrsMp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트나 사진을 데이터베이스에 저장할 수는 있다. 다만 여기서는 보편적인 사례를 설명했다. 데이터 베이스 중에는 구조적이지 않은 데이터를 저장하는 데 편리하도록 발전한 것이 많다. 대표적으로 NoSQL 데이터베이스는 엑셀이나 CSV에 담기 어려운 텍스트나 JSON 데이터를 저장하는 데 용잉하다."
      ],
      "metadata": {
        "id": "KS94PHxUtIxe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "지금까지 배운 머신러닝 알고리즘은 정형 데이터에 잘 맞는다. 그중에 정형 데이터를 다루는 데 가장 뛰어난 성과를 내는 알고리즘이 앙상블 학습(ensemble learning)이다. 이 알고리즘은 대부분 결정 트리를 기반으로 만들어져 있다. 이 절에서 배울 알고리즘들이 앙상블 학습에 속한다. </br>\n",
        "그럼 비정형 데이터는 어떤 알고리즘을 사용해야 할까? 바로 7장에서 배울 신경망 알고리즘이다. 비정형 데이터는 규칙성을 찾기 어려워 전통적인 머신러닝 방법으로는 모델을 만들기 까다롭다. 하지만 신경망 알고리즘의 놀라운 발전 덕분에 사진을 인식하고 텍스트를 이해하는 모델을 만들 수 있다. </br>\n",
        "이제 사이킷런에서 제공하는 정형 데이터의 끝판왕인 앙상블 학습 알고리즘을 알아보자!"
      ],
      "metadata": {
        "id": "qaQK5kYdthGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **랜덤 포레스트(Random Forest)**\n",
        "  - 랜덤 포레스트는 앙상블 학습의 대표 주자 중 하나로 안정적인 성능 덕분에 널리 사용되고 있다. 앙상블 학습을 적용할 때 가장 먼저 랜덤 포레스트를 시도해 보길 권장한다.\n",
        "  - 이름 자체로 유추할 수 있듯이 랜덤 포레스트는 결정 트리를 랜덤하게 만들어 결정 트리(나무) **숲**을 만든다. 그리고 각 결정 트리의 예측을 사용해 최종 예측을 만든다. 그럼 랜덤 포레스트가 어떻게 숲을 구성하는지 관찰해보자\n",
        "  - 이 절은 사이킷런에 구현된 앙상블 학습 알고리즘을 기준으로 설명한다. 머신러닝 라이브러리마다 구현 방식에 조금씩 차이가 있을 수 있다.\n",
        "  - 랜덤 포레스트는 각 트리를 훈련하기 위한 데이터를 랜덤하게 만드는데, 이 데이터를 만드는 방법이 독특하다. 우리가 입력한 훈련 데이터에서 랜덤하게 샘플을 추출하여 훈련 데이터를 만든다. 이 때 한 샘플이 중복되어 추출될 수도 있다. 중복을 허락하여 만든 샘플 -> **부트스트램 샘플(bootstrap sample)**\n",
        "    - 부트스트랩 샘플은 훈련 세트의 크기와 같게 만든다.\n",
        "    - 1,000개의 샘플이 들어있는 가방에서 중복하여 1,000개의 샘플을 뽑는다.\n",
        "    - 각 노드를 분할할 때 전체 특성 중에서 일부 특성을 무작위로 고른 다음 이 중에서 최선의 분할을 찾는다.\n",
        "  - 분류 모델인 RandomForestClassifier는 기본적으로 전체 특성 개수의 제곱근만틈의 특성을 선택한다. 즉 4개의 특성이 있다면 노드마다 2개를 랜덤하게 선택하여 사용한다. 다만 회귀 모델인 RandomForestRegressor는 전체 특성을 사용한다.\n",
        "  - 사이킷런의 랜덤 포레스트는 기본적으로 100개의 결정 트리를 이런 방식으로 훈련한다.\n",
        "  - 분류일 때는 각 트리의 클래스별 확률을 평균하여 가장 높은 확률을 가진 클래스를 예측으로 삼는다. 회귀일 때는 단순히 각 트리의 예측을 평균한다.\n",
        "  - 랜덤 포레스트는 랜덤하게 선택한 샘플과 특성을 사용하기 때문에 훈련 세트에 과대적합되는 것을 막아주고 검증 세트와 테스트 세트에서 안정적인 성능을 얻을 수 있다. 종종 기본 매개변수 설정만으로도 아주 좋은 결과를 낸다.\n"
      ],
      "metadata": {
        "id": "zMAjMszHuNbd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vpuW5zbXqv-U"
      },
      "outputs": [],
      "source": [
        "# 사이킷런의 RandomForestClassifier 클래스를 화이트 와인을 분류하는 문제에 적용해 보자\n",
        "# 와인 데이터셋을 판다스로 불러오고 훈련 세트와 테스트 세트로 나눈다\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "wine = pd.read_csv('https://bit.ly/wine_csv_data')\n",
        "data = wine[['alcohol', 'sugar', 'pH']].to_numpy()\n",
        "target = wine['class'].to_numpy()\n",
        "train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**cross_validate()** 함수를 사용해 교차 검증을 수행해보자. </br>\n",
        "RandomForestClassifier는 기본적으로 100개의 결정 트리를 사용하므로 n_jobs 매개변수를 -1로 지정하여 모든 CPU 코어를 사용하는 것이 좋다. cross_validate() 함수의 n_jobs 매개변수도 -1로 지정하여 최대한 병렬로 교차 검증을 수행하자. 또 **return_train_score 매개변수를 True로 지정하면 검증 점수분만 아니라 훈련 세트에 대한 점수도 같이 반환**한다. 훈련 세트와 검증 세트의 점수를 비교하면 과대적합을 파악하는 데 용이하다. (return_train_score 매개변수의 기본값은 False이다.)"
      ],
      "metadata": {
        "id": "XMewkGKPgV3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
        "scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lbp_MVS9f9X0",
        "outputId": "692ebf15-9a5e-455e-b238-ad6518fc4488"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9973541965122431 0.8905151032797809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "출력된 결과를 보면 훈련 세트에 다소 과대적합된 것 같다. 여기에서는 알고리즘을 조사하는 것이 목적이므로 매개변수를 더 조정하지 않도록 하겠다. (사실 이 예제는 매우 간단하고 특성이 많지 않아 그리드 서치를 사용하더라도 하이퍼파라미터 튜닝의 결과가 크게 나아지지 않는다.)"
      ],
      "metadata": {
        "id": "BxPIhtRbiOb1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**랜덤 포레스트는 결정 트리의 앙상블**이기 때문에 **DecisionTreeClassifier**가 제공하는 중요한 매개변수를 모두 제공한다. **criterion, max_depth, max_features, min_samples_split, min_impurity_decrease, min_samples_leaf** 등이다. 또한 결정 트리의 큰 장점 중 하나인 **특성 중요도**를 계산한다. 랜덤 포레스트의 특성 중요도는 각 결정 트리의 특성 중요도를 취합한 것이다."
      ],
      "metadata": {
        "id": "tpyHLqBYi1DC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 랜덤 포레스트 모델을 훈련 세트에 훈련한 후 특성 중요도를 출력해보자\n",
        "\n",
        "rf.fit(train_input, train_target)\n",
        "print(rf.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVxH1KAihZl8",
        "outputId": "e941b72f-8eb2-4643-8209-4040dc7b1d34"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.23167441 0.50039841 0.26792718]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "결과를 앞의 1절 '결정 트리'에서 만든 특성 중요도와 비교해 보자. [0.12345626, 0.86862934, 0.0079144] </br>\n",
        "각각 [알코올 도수, 당도, pH]였는데, 두 번째 특성인 당도의 중요도가 감소하고 알코올 도수와 pH 특성의 중요도가 조금 상승했다. </br>\n",
        "이런 이유는 랜덤 포레스트가 특성의 일부를 랜덤하게 선택하여 결정 트리를 훈련하기 때문이다. 그 결과 **하나의 특성에 과도하게 집중하지 않고 좀 더 많은 특성이 훈련에 기여할 기회**를 얻는다. 이는 과대적합을 줄이고 일반화 성능을 높이는 데 도움이 된다."
      ],
      "metadata": {
        "id": "IHqWsieSj3Z-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomForestClassifier에는 재미있는 기능이 하나 더 있는데, 자체적으로 모델을 평가하는 점수를 얻을 수 있다. 랜덤 포레스트는 훈련 세트에서 중복을 허용하여 부트스트랩 샘플을 만들어 결정 트리를 훈련한다고 했다. 이때 부트스트랩 샘플에 포함되지 않고 남는 샘플이 있다. 이런 샘플을 **OOB(out of bag)** 샘플이라고 한다. **이 남는 샘플을 사용하여 부트스트랩 샘플로 훈련한 결정 트리를 평가**할 수 있다. 마치 **검증 세트의 역할**을 한다!"
      ],
      "metadata": {
        "id": "Tw2Vi0rAlgJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 점수를 얻으려면 RandomForestClassifier 클래스의 **oob_score 매개변수를 True로 지정**해야 한다(이 매개변수의 기본값은 False이다). 이렇게 하면 랜덤 포레스트는 각 결정 트리의 OOB 점수를 평균하여 출력한다. **oob_score=True로 지정하고 모델을 훈련하여 OOB 점수를 출**력해 보자."
      ],
      "metadata": {
        "id": "38q6cxSbnNpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)\n",
        "rf.fit(train_input, train_target)\n",
        "print(rf.oob_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMeEiQMUjsYf",
        "outputId": "dd3722a7-6d61-4b1c-b388-39b1c90bf225"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8934000384837406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "교차 검증에서 얻는 점수와 매우 비슷한 결과를 얻었다. OOB 점수를 사용하면 교차 검증을 대신할 수 있어서 결과적으로 훈련 세트에 더 많은 샘플을 사용할 수 있다. </br>\n",
        "다음 알아볼 앙상블 학습은 랜덤 포레스트와 아주 비슷한 엑스트라 트리이다."
      ],
      "metadata": {
        "id": "hQXVc7aSo_tU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **엑스트라 트리(Extra Trees)**\n",
        "  - 엑스트라 트리는 랜덤 포레스트와 매우 비슷하게 동작한다. 기본적으로 100개의 결정 트리를 훈련한다. 랜덤 포레스트와 동일하게 결정 트리가 제공하는 대부분의 매개변수를 지원한다. 또한 전체 특성 중에 일부 특성을 랜덤하게 선택하여 노드를 분할하는 데 사용한다.\n",
        "  - **랜덤 포레스트와 엑스트라 트리의 차이점은 부트스트랩 샘플을 사용하지 않는다는 점**이다. 즉 각 결정 트리를 만들 때 전체 훈련 세트를 사용한다. 대신 노드를 분할할 때 가장 좋은 분할을 찾는 것이 아니라 **무작위로 분할**한다!\n",
        "  - 실은 2절의 확인 문제에서 DecisionTreeClassifier의 splitter 매개변수를 'random'으로 지정했는데, **엑스트라 트리가 사용하는 결정 트리가 바로 splitter='random'인 결정 트리**이다.\n",
        "  - 하나의 결정 트리에서 특성을 무작위로 분할한다면 성능이 낮아지겠지만 많은 트리를 앙상블 하기 때문에 과대적합을 막고 검증 세트의 점수를 높이는 효과가 있다.\n",
        "  - 사이킷런에서 제공하는 엑스트라 트리는 ExtraTreesClassifier이다."
      ],
      "metadata": {
        "id": "z_B-a9ngpKXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "et = ExtraTreesClassifier(n_jobs=-1, random_state=42)\n",
        "scores = cross_validate(et, train_input, train_target,\n",
        "                        return_train_score=True, n_jobs=-1)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kl8qa1b1KfYd",
        "outputId": "79bd6d3a-77d3-4841-aff6-d7791cdecf29"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9974503966084433 0.8887848893166506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "랜덤 포레스트와 비슷한 결과를 얻었다. 이 예제는 특성이 많지 않아 두 모델의 차이가 크지 않다. </br>\n",
        "보통 엑스트라 트리가 무작위성이 좀 더 크기 때문에 랜덤 포레스트보다 더 많은 결정 트리를 훈련해야 한다. 하지만 랜덤하게 노드를 분할하기 때문에 빠른 계산 속도가 엑스트라 트리의 장점이다. (결정 트리는 최적의 분할을 찾는 데 시간을 많이 소모한다. 특히 고려해야 할 특성의 개수가 많을 때 더 그렇다. 만약 무작위로 나눈다면 훨씬 빨리 트리를 구성할 수 있다.)"
      ],
      "metadata": {
        "id": "MpyILkRkK8I0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "엑스트라 트리도 랜덤 포레스트와 마찬가지로 특성 중요도를 제공한다. 순서는 [알코올 도수, 당도, pH]인데, 결과를 보면 엑스트라 트리도 결정 트리보다 당도에 대한 의존성이 작다."
      ],
      "metadata": {
        "id": "9h7iviXmLkua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "et.fit(train_input, train_target)\n",
        "print(et.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXd3BkxAK0CZ",
        "outputId": "a1fbc89b-a5e7-4565-90b5-2681c7f193ee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.20183568 0.52242907 0.27573525]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "엑스트라 트리의 회귀 버전은 ExtraTreesRegressor 클래스이다. </br>\n",
        "지금까지 비슷하지만 조금 다른 2개의 앙상블 학습을 알아보았다. 다음에는 이 둘과 다른 방식을 사용하는 앙상블 학습을 알아보자."
      ],
      "metadata": {
        "id": "VExfBSetL0xR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **그레이디언트 부스팅(gradient boosting)**\n",
        "  - 그래이디언트 부스팅은 **깊이가 얕은 결정 트리**를 사용하여 **이전 트리의 오차를 보완하는 방식**으로 앙상블을 학습하는 방법이다.\n",
        "  - 사이킷런의 GradientBoostingClassifier는 기본적으로 깊이가 3인 결정 트리를 100개 사용한다. 깊이가 얕은 결정 트리를 사용하기 때문에 과대적합에 강하고 일반적으로 높은 일반화 성능을 기대할 수 있다.\n",
        "  - 경사 하강법을 사용하여 트리를 앙상블에 추가한다. 분류에서는 로지스틱 손실 함수를 사용하고 회귀에서는 평균 제곱 오차 함수를 사용한다.\n",
        "  - 학습률 매개변수로 속도를 조절한다.\n",
        "  - 사이킷런에서 제공하는 GradientBoostingClassifier를 사용하자."
      ],
      "metadata": {
        "id": "CjQpR7tqMCGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 사이킷런에서 제공하는 GradientBoosting Classifier를 사용해 와인 데이터셋의 교차 검증 점수를 확인해보자\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "scores = cross_validate(gb, train_input, train_target,\n",
        "                        return_train_score=True, n_jobs=-1)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzmqxpZxLxkW",
        "outputId": "85794c6f-b4b8-4de8-a9e0-17f6a4e066b1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8881086892152563 0.8720430147331015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "거의 과대적합이 되지 않는다! 그레이디언트 부스팅은 결정 트리의 개수를 늘려도 과대적합에 매우 강하다. 학습률을 증가시키고 트리의 개수를 늘리면 조금 더 성능이 향상될 수 있다."
      ],
      "metadata": {
        "id": "XkI8Z7c5OadK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)\n",
        "scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjkPBqbDOOJY",
        "outputId": "c0031a93-8972-42ee-b3dd-18b929905ec9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9464595437171814 0.8780082549788999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "결정 트리의 개수를 500개로 5배나 늘렸지만 과대적합을 잘 억제하고 있다. 학습률 learning_rate의 기본값은 0.1이다. 그레이디언트 부스팅도 특성 중요도를 제공한다. 결과에서 볼 수 있듯이 그레이디언트 부스팅이 랜덤 포레스트보다 일부 특성(당도)에 더 집중한다."
      ],
      "metadata": {
        "id": "TU1lt0tqcS75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gb.fit(train_input, train_target)\n",
        "print(gb.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taXDv5y9cjsd",
        "outputId": "3f013cb5-d379-4f18-8603-bf66581f1f73"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.15872278 0.68010884 0.16116839]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "트리 훈련에 사용할 훈련 세트의 비율을 정하는 subsample. 이 매개변수의 기본값은 1.0으로 전체 훈련 세트를 사용한다. 하지만 subsample이 1보다 작으면 훈련 세트의 일부를 사용한다. 이는 마치 경사 하강법 단계마다 일부 샘플을 랜덤하게 선택하여 진행하는 확률적 경사 하강법이나 미니배치 경사 하강법과 비슷하다."
      ],
      "metadata": {
        "id": "aLdyjTeOc0xy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "일반적으로 그레이디언트 부스팅이 랜덤 포레스트보다 조금 더 높은 성능을 얻을 수 있다. 하지만 순서대로 트리를 추가하기 때문에 훈련 속도가 느리다. 즉 GradientBoostingClassifier에는 n_jobs 매개변수가 없다. 그레이디언트 부스팅의 회귀 버전은 GradientBoostingRegressor이다. 그레이디언트 부스팅의 속도와 성능을 더욱 개선한 것이 다음에 살펴볼 히스토그램 기반 그레이디언트 부스팅이다."
      ],
      "metadata": {
        "id": "snDaKdExdHHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **히스토그램 기반 그레이디언트 부스팅(Histogram-based Gradient Boosting)**\n",
        "  - 히스토그램 기반 그레이디언트 부스팅(Histogram-based Gradient Boosting)은 정형 데이터를 다루는 머신러닝 알고리즘 중에 가장 인기가 높은 알고리즘이다.\n",
        "  - 히스토그램 기반 그레이디언트 부스팅은 먼저 입력 특성을 256개의 구간으로 나눈다. 따라서 노드를 분할할 때 최적의 분할을 매우 빠르게 찾을 수 있다. 히스토그램 기반 그레이디언트 부스팅은 256개의 구간 중에서 하나를 떼어 놓고 누락된 값을 위해서 사용한다. 따라서 입력에 누락된 특성이 있더라도 이를 따로 전처리할 필요가 없다.\n",
        "  - 사이킷런의 히스토그램 기반 그레이디언트 부스팅 클래스는 **HisGradientBoostingClassifier**이다. 일반적으로 HisGradientBoostingClassifier는 기본 매개변수에서 안정적인 성능을 얻을 수 있다. HisGradientBoostingClassifier에는 트리의 개수를 지정하는데 n_estimators 대신 부스팅 반복 횟수를 지정하는 max_iter를 사용한다. 성능을 높이려면 max_iter 매개변수를 테스트해보자."
      ],
      "metadata": {
        "id": "1BcT-NfidYuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 와인 데이터셋에 HisGradientBoostingClassifier 클래스 적용\n",
        "\n",
        "#from sklearn.experimental import enable_hist_gradient_boosting - 사이킷런 1.0에서 히스토그램 기반 부스팅이 experimental 모듈 아래에서 벗어났다.\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "hgb = HistGradientBoostingClassifier(random_state=42)\n",
        "scores = cross_validate(hgb, train_input, train_target, return_train_score=True)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvfy1Tu2ed46",
        "outputId": "afb01ff8-3397-42b0-8fb2-9ac0fdc280b9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9321723946453317 0.8801241948619236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "과대적합을 잘 억제하면서 그레이디언트 부스팅보다 조금 더 높은 성능을 제공한다. </br>\n",
        "히스토그램 기반 그레이디언트 부스팅의 특성 중요도를 계산하기 위해 permutation_importance() 함수를 사용하자. 이 함수는 특성을 하나씩 랜덤하게 섞어서 모델의 성능이 변화하는지를 관찰하여 어떤 특성이 중요한지를 계산한다. 훈련 세트뿐만 아니라 테스트 세트에도 적용할 수 있고 사이킷런에서 제공하는 추정기 모델에 모두 사용할 수 있다."
      ],
      "metadata": {
        "id": "M4WhOG1OfVV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "먼저 히스토그램 기반 그레이디언트 부스팅 모델을 훈련하고 훈련 세트에서 특성 중요도를 계산해보자. n_repeats 매개변수는 랜덤하게 섞을 횟수를 지정한다. 여기서는 10으로 지정하겠다. 기본값은 5이다."
      ],
      "metadata": {
        "id": "zV1lhSxXf-7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "hgb.fit(train_input, train_target)\n",
        "result = permutation_importance(hgb, train_input, train_target, n_repeats=10, random_state=42, n_jobs=-1)\n",
        "print(result.importances_mean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brZk0cfdfDhQ",
        "outputId": "e600116b-498d-4a4e-a5f4-2b74deaf98c0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.08876275 0.23438522 0.08027708]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "permutation_importance() 함수가 반환하는 객체는 반복하여 얻은 특성 중요도(importances), 평균(importances_mean), 표준편차(importances_std)를 담고 있다. 평균을 출력해보면 랜덤 포레스트와 비슷한 비율임을 알 수 있다."
      ],
      "metadata": {
        "id": "bq46-BQ0glLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = permutation_importance(hgb, test_input, test_target, n_repeats=10, random_state=42, n_jobs=-1) # 테스트 세트에서 특성 중요도 계산\n",
        "print(result.importances_mean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVu7zppDGBJP",
        "outputId": "731bf46e-2f59-4cad-a84c-0e7e81f2a93a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.05969231 0.20238462 0.049     ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "테스트 세트의 결과를 보면 그레이디언트 부스팅과 비슷하게 조금 더 당도에 집중하고 있다는 것을 알 수 있다. 이런 분석을 통해 모델을 실전에 투입했을 때 어떤 특성에 관심을 둘지 예상할 수 있다."
      ],
      "metadata": {
        "id": "xKcz_FoEGfP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hgb.score(test_input, test_target) # 테스트 세트에서의 성능을 최종적으로 확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3ABURBGGoUq",
        "outputId": "977a9857-2d6e-4c7c-901e-a13f542531b7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8723076923076923"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "테스트 세트에서는 약 87% 정확도를 얻었다. 실전에 투입하면 성능은 이보다는 조금 더 낮을 것이다. 앙상블 모델은 확실히 단일 결정 트리보다 좋은 결과를 얻을 수 있다. (2절의 랜덤 서치에서 테스트 정확도는 86% 였다.)"
      ],
      "metadata": {
        "id": "LwDdxxiFbpyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "히스토그램 기반 그레이디언트 부스팅의 회귀 버전은 HistGradientBoostingRegressor 클래스에 구현되어 있다. 사이킷런에서 제공하는 히스토그램 기반 그레이디언트 부스팅이 비교적 새로운 기능이다. 하지만 사이킷런 말고도 그레이디언트 부스팅 알고리즘을 구현한 라이브러리가 여럿 있다."
      ],
      "metadata": {
        "id": "oUHK3TvTcOMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "가장 대표적인 라이브러리는 **XGBoost**이다. 놀랍게도 이 라이브러리도 코랩에서 사용할 수 있을 뿐만 아니라 사이킷런의 cross_validate() 함수와 함께 사용할 수도 있다. XGBoost는 다양한 부스팅 알고리즘을 지원한다. tree_method 매개변수를 'hist'로 지정하면 히스토그램 기반 그레이디언트 부스팅을 사용할 수 있다."
      ],
      "metadata": {
        "id": "z-URCNspccmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost를 사용해 와인 데이터의 교차 검증 점수를 확인해보자\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(tree_method='hist', random_state=42)\n",
        "scores = cross_validate(xgb, train_input, train_target, return_train_score=True)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqR-Xo9abmjN",
        "outputId": "97ff8ba8-a1b6-4789-fa31-59427c764242"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9558403027491312 0.8782000074035686\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "널리 사용하는 또 다른 히스토그램 기반 그레이디언트 부스팅 라이브러리는 마이크로소프트에서 만든 LightGBM이다. LightGBM은 빠르고 최신 기술을 많이 적용하고 있어 인기가 점점 높아지고 있다."
      ],
      "metadata": {
        "id": "yfh6DcstdT12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LightGBM을 사용해 와인 데이터의 교차 검증 점수를 확인해보자\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "lgb = LGBMClassifier(random_state=42)\n",
        "scores = cross_validate(lgb, train_input, train_target,\n",
        "                        return_train_score=True, n_jobs=-1)"
      ],
      "metadata": {
        "id": "Ok0-8xt7dLUo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwWE4rDFd3Wr",
        "outputId": "cbc0a81d-aa6a-4767-81e1-d4ec7c91f102"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.935828414851749 0.8801251203079884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "사이킷런의 히스토그램 기반 그레이디언트 부스팅이 LightGBM에서 영향을 많이 받았다. 이제 히스토그램 기반 그레이디언트 부스팅까지 4개의 앙상블을 모두 다루어 보았다."
      ],
      "metadata": {
        "id": "KibWkUsdeJqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 앙상블 학습 보고서\n",
        "  - 앙상블 학습은 정형 데이터에서 가장 뛰어난 성능을 내는 머신러닝 알고리즘 중 하나이다.\n",
        "  - 대표적인 앙상블 학습은 다음과 같다.\n",
        "    - 사이킷런\n",
        "      - **랜덤 포레스트**: 부트스트랩 샘플 사용. 대표 앙상블 학습 알고리즘임\n",
        "      - **엑스트라 트리**: 결정 트리의 노드를 랜덤하게 분할함\n",
        "      - **그레이디언트 부스팅**: 이전 트리의 손실을 보완하는 식으로 얕은 결정 트리를 연속하여 추가함\n",
        "      - **히스토그램 기반 그레이디언트 부스킹**: 훈련 데이터를 256개 정수 구간으로 나누어 빠르고 높은 성능을 냄\n",
        "    - 그 외 라이브러리\n",
        "      - XGBoost\n",
        "      - LightGBM"
      ],
      "metadata": {
        "id": "PNErnbageWey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 앙상블 학습 알고리즘 다시 한번 정리!\n",
        "  - 랜덤 포레스트\n",
        "    - 가장 대표적인 앙상블 학습 알고리즘\n",
        "    - 랜덤 포레스트는 결정 트리를 훈련하기 위해 부트스트랩 샘플을 만들고 전체 특성 중 일부를 랜덤하게 선택하여 결정 트리를 만든다.\n",
        "  - 엑스트라 트리\n",
        "    - 랜덤 포레스트와 매우 비슷하지만 부트스트랩을 샘플을 사용하지 않고 노드를 분할할 때 최선이 아니라 랜덤하게 분할한다. 이런 특징 때문에 랜덤 포레스트보다 훈련 속도가 빠르지만 보통 더 많은 트리가 필요하다.\n",
        "  - 그레이디언트 부스팅\n",
        "    - 깊이가 얇은 트리를 연속적으로 추가하여 손실 함수를 최소화하는 앙상블 방법\n",
        "    - 성능이 뛰어나지만 병렬로 훈련할 수 없기 때문에 랜덤 포레스트나 엑스트라 트리보다 훈련 속도가 조금 느리다.\n",
        "    - 학습률 매개변수를 조정하여 모델의 복잡도를 제어할 수 있다. 학습률 매개변수가 크면 복잡하고 훈련 세트에 과대적합된 모델을 얻을 수 있다.\n",
        "  - 히스토그램 기반 그레이디언트 부스팅\n",
        "    - 가장 뛰어난 앙상블 학습으로 평가받는 히스토그램 기반 그레이디언트 부스팅 알고리즘\n",
        "    - 훈련 데이터를 256개의 구간으로 변환하여 사용하기 때문에 노드 분할 속도가 매우 빠르다.\n",
        "    - 코랩에는 사이킷런뿐만 아니라 히스토그램 기반 그레이디언트 부스팅 라이브러리인 XGBoost와 LightGBM이 이미 설치되어 있어 바로 시험해 볼 수 있다."
      ],
      "metadata": {
        "id": "wJefRQSlfF-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 절에서 다양한 앙상블 학습 방법을 배워 보았다. 앙상블 학습과 그리드 서치, 랜덤 서치를 사용한 하이퍼파라미터 튜닝을 사용하면 최고 수준의 성능을 내는 머신러닝 모델을 얻을 수 있다."
      ],
      "metadata": {
        "id": "t_34GDVagezT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "지금까지는 입력과 타깃이 준비된 문제를 풀었다. 이런 머신러닝 분야를 지도 학습(supervised learning)이라고 부른다. 타깃이 없다면 어떨까요? 이때에도 유용한 무언가를 학습할 수 있을까? 다음 장에서 이에 대해 배워 보자."
      ],
      "metadata": {
        "id": "qfGalNgigosQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 마무리\n",
        "  - 앙상블 학습은 더 좋은 예측 결과를 만들기 위해 여러 개의 모델을 훈련하는 머신러닝 알고리즘을 말한다.\n",
        "  - 랜덤 포레스트는 대표적인 결정 트리 기반의 앙상블 학습 방법이다. 부트스트랩 샘플을 사용하고 랜덤하게 일부 특성을 선택하여 트리를 만드는 것이 특징이다.\n",
        "  - 엑스트라 트리는 랜덤 포레스트와 비슷하게 결정 트리를 사용하여 앙상블 모델을 만들지만 부트스트랩 샘플을 사용하지 않는다. 대신 랜덤하게 노드를 분할해 과대적합을 감소시킨다.\n",
        "  - 그레이디언트 부스팅은 랜덤 포레스트나 엑스트라 트리와 달리 결정 트리를 연속적으로 추가하여 손실 함수를 최소화하는 앙상블 방법이다. 이런 이유로 훈련 속도가 조금 느리지만 더 좋은 성능을 기대할 수 있다. 그레이디언트 부스팅의 속도를 개선한 것이 히스토그램 기반 그레이디언트 부스팅이며 안정적인 결과와 높은 성능으로 인기가 높다."
      ],
      "metadata": {
        "id": "oSbAgX-5MqE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 핵심 패키지와 함수\n",
        "  - scikit-learn\n",
        "    - RandomForestClassifier는 랜덤 포레스트 분류 클래스이다.\n",
        "    - ExtraTreesClassifer는 엑스트라 트리 분류 클래스이다.\n",
        "    - GradientBoostingClassifier는 그레이디언트 부스팅 분류 클래스이다.\n",
        "    - HistGradientBoostingClassifier는 히스토그램 기반 그레이디언트 부스팅 분류 클래스이다."
      ],
      "metadata": {
        "id": "vb6QisBZNRef"
      }
    }
  ]
}